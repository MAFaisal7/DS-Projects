{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "predict.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8ZiXUgbVKzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FN = 'predict'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf_JgmlvVKzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['THEANO_FLAGS'] = 'device=cpu,floatX=float32'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iybfLO5GVKz4",
        "colab_type": "code",
        "colab": {},
        "outputId": "f48d55e6-cb41-4d8e-f958-5d7bf509b86d"
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.6'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQoImfyMVK0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FN0 = 'json_news-5000-glove-vocab-embedding'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHW6SGHiVK0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FN1 = 'train-5000-2'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mKYOIihVK0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_train_samples = 5000\n",
        "nb_val_samples = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HrxTBOIVK0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlend = 75 # 0 - if we dont want to use description at all\n",
        "maxlenh = 16\n",
        "maxlen = maxlend + maxlenh\n",
        "rnn_size = 64\n",
        "rnn_layers = 2  # match FN1\n",
        "# rnn_layers = 3\n",
        "batch_norm = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcfndSuGVK0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation_rnn_size = 40 if maxlend else 0\n",
        "\n",
        "lambda_dim = 2*(rnn_size - activation_rnn_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPJSe2wQVK0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training parameters\n",
        "seed = 42\n",
        "p_W, p_U, p_dense, weight_decay = 0, 0, 0, 0\n",
        "optimizer = 'rmsprop'\n",
        "batch_size=64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opL0Mpd5VK0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('pickles/%s.pickle'%FN0, 'rb') as fp:\n",
        "    embedding, idx2word, word2idx, glove_idx2idx = pickle.load(fp)\n",
        "vocab_size, embedding_size = embedding.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG-7O5YKVK0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_unknown_words = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml_P7XtCVK0p",
        "colab_type": "code",
        "colab": {},
        "outputId": "3abbe2df-8596-46e1-9103-95fe2e6f8723"
      },
      "source": [
        "print('dimension of embedding space for words',embedding_size)\n",
        "print('vocabulary size', vocab_size, 'the last %d words can be used as place holders for unknown/oov words'%nb_unknown_words)\n",
        "print('total number of different words',len(idx2word), len(word2idx))\n",
        "print('number of words outside vocabulary which we can substitue using glove similarity', len(glove_idx2idx))\n",
        "print('number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov)',len(idx2word)-vocab_size-len(glove_idx2idx))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dimension of embedding space for words 100\n",
            "vocabulary size 39453 the last 10 words can be used as place holders for unknown/oov words\n",
            "total number of different words 39455 39455\n",
            "number of words outside vocabulary which we can substitue using glove similarity 8890\n",
            "number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov) -8888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDSt5ZPPVK0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(nb_unknown_words):\n",
        "    idx2word[vocab_size-1-i] = '<%d>'%i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncTJ5j1vVK0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(vocab_size-nb_unknown_words, len(idx2word)):\n",
        "    idx2word[i] = idx2word[i]+'^'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-rKryzYVK00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "empty = 0\n",
        "eos = 1\n",
        "idx2word[empty] = '_'\n",
        "idx2word[eos] = '~'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZbzU9v7VK02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "import random, sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzfFFg3-VK05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prt(label, x):\n",
        "    print(label+':'),\n",
        "    for w in x:\n",
        "        print(idx2word[w]),\n",
        "    print"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6URza8xsVK09",
        "colab_type": "code",
        "colab": {},
        "outputId": "13502863-03b0-4be5-eda9-39525f8babcf"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation, Dropout, RepeatVector\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.regularizers import l2\n",
        "from keras.layers.core import Lambda\n",
        "import keras.backend as K\n",
        "K.backend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tensorflow'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 275
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM2vg_X4VK1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# seed weight initialization\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfjBrGkFVK1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regularizer = l2(weight_decay) if weight_decay else None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g5UUpPWVK1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_model = Sequential()\n",
        "# from keras.layers import InputLayer\n",
        "# model_input = InputLayer(input_shape=(maxlen,))\n",
        "# rnn_model.add(model_input)\n",
        "# rnn_model.output.tag.test_value = np.random.randint(vocab_size,size=(batch_size,maxlen)).astype('float32')\n",
        "rnn_model.add(Embedding(vocab_size, embedding_size,\n",
        "                        input_length=maxlen,\n",
        "#                         batch_input_shape=(batch_size,maxlen),\n",
        "                        embeddings_regularizer=regularizer, weights=[embedding], mask_zero=True,\n",
        "                        name='embedding_1'))\n",
        "for i in range(rnn_layers):\n",
        "    lstm = LSTM(rnn_size, return_sequences=True, # batch_norm=batch_norm,\n",
        "                kernel_regularizer=regularizer, recurrent_regularizer=regularizer,\n",
        "                bias_regularizer=regularizer, dropout=p_W, recurrent_dropout=p_U,\n",
        "                name='lstm_%d'%(i+1)\n",
        "                  )\n",
        "    rnn_model.add(lstm)\n",
        "    rnn_model.add(Dropout(p_dense, name='dropout_%d'%(i+1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_gkExSXVK1T",
        "colab_type": "code",
        "colab": {},
        "outputId": "58cd416c-c580-4bb9-e664-4e1dd34e3d3c"
      },
      "source": [
        "rnn_model.load_weights('pickles/train-weights-lines_5000-iter_100-layers_2_iter100.hdf5',by_name=True)\n",
        "rnn_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 91, 100)           3945300   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 91, 64)            42240     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 91, 64)            0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 91, 64)            33024     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 91, 64)            0         \n",
            "=================================================================\n",
            "Total params: 4,020,564\n",
            "Trainable params: 4,020,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD5NiGSHVK1W",
        "colab_type": "code",
        "colab": {},
        "outputId": "ee232679-ad1e-4cc7-8e50-c138f7a87ff3"
      },
      "source": [
        "print(rnn_model.layers[0].get_weights())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[-0.1773875 ,  0.6372642 ,  0.32801583, ..., -0.10244963,\n",
            "        -0.67100906, -0.55440164],\n",
            "       [-0.89117724,  0.42207116, -0.25733128, ...,  0.6377615 ,\n",
            "         0.6397168 ,  0.3933113 ],\n",
            "       [-0.04279218, -0.21113484,  0.7327986 , ..., -0.15707017,\n",
            "         0.8271285 ,  0.25070208],\n",
            "       ...,\n",
            "       [ 0.48706606,  0.6691793 , -0.0882058 , ...,  0.3558768 ,\n",
            "        -0.14876254, -0.14990547],\n",
            "       [-0.67171794, -0.09513056, -0.6988457 , ..., -0.25448114,\n",
            "         0.50731444,  0.42698762],\n",
            "       [-0.50040406,  0.13563907, -0.3586453 , ..., -0.64897496,\n",
            "        -0.3132541 , -0.43104267]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oloNrCheVK1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "with h5py.File('pickles/train-weights-lines_5000-iter_100-layers_2_iter100.hdf5', mode='r') as f:\n",
        "    if 'layer_names' not in f.attrs and 'model_weights' in f:\n",
        "        f = f['model_weights']\n",
        "    weights = [np.copy(v) for v in f['time_distributed_1']['time_distributed_1'].values()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyWAoE7yVK1b",
        "colab_type": "code",
        "colab": {},
        "outputId": "9129ff73-1a8c-48f0-d912-3aeba7b805b5"
      },
      "source": [
        "# kernel, bias\n",
        "weights = np.array([weights[1], weights[0]])\n",
        "list(map(lambda x: x.shape, weights))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(48, 39453), (39453,)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBZ6GeamVK1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_context(X, mask, n=activation_rnn_size, maxlend=maxlend, maxlenh=maxlenh):\n",
        "    desc, head = X[:,:maxlend,:], X[:,maxlend:,:]\n",
        "    head_activations, head_words = head[:,:,:n], head[:,:,n:]\n",
        "    desc_activations, desc_words = desc[:,:,:n], desc[:,:,n:]\n",
        "    print(type(mask))\n",
        "    # RTFM http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor.batched_tensordot\n",
        "    # activation for every head word and every desc word\n",
        "    activation_energies = K.batch_dot(head_activations, desc_activations, axes=(2,2))\n",
        "    # make sure we dont use description words that are masked out\n",
        "    activation_energies = activation_energies + -1e20*K.expand_dims(1.-K.cast(mask[:, :maxlend],'float32'),1)\n",
        "    \n",
        "    # for every head word compute weights for every desc word\n",
        "    activation_energies = K.reshape(activation_energies,(-1,maxlend))\n",
        "    activation_weights = K.softmax(activation_energies)\n",
        "    activation_weights = K.reshape(activation_weights,(-1,maxlenh,maxlend))\n",
        "\n",
        "    # for every head word compute weighted average of desc words\n",
        "    desc_avg_word = K.batch_dot(activation_weights, desc_words, axes=(2,1))\n",
        "    return K.concatenate((desc_avg_word, head_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYFLm98yVK1l",
        "colab_type": "code",
        "colab": {},
        "outputId": "7ed681d1-8e29-406b-8002-014d2957e197"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(rnn_model)\n",
        "model.summary()\n",
        "if activation_rnn_size:\n",
        "    model.add(Lambda(simple_context,\n",
        "                     mask = lambda inputs, mask: mask[:,maxlend:],\n",
        "                     output_shape = lambda input_shape: (input_shape[0], maxlenh, lambda_dim),\n",
        "                     name='simplecontext_1'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential_15 (Sequential)   (None, 91, 64)            4020564   \n",
            "=================================================================\n",
            "Total params: 4,020,564\n",
            "Trainable params: 4,020,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "<class 'tensorflow.python.framework.ops.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fUXQiNGVK1o",
        "colab_type": "code",
        "colab": {},
        "outputId": "a7e57034-5ed9-48cd-c02a-95ed89e9d6c0"
      },
      "source": [
        "lambda_dim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_4sZN0BVK1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we are not going to fit so we dont care about loss and optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EONSD1ceVK1u",
        "colab_type": "code",
        "colab": {},
        "outputId": "620cfaba-9695-4a10-f34c-0fc4ac4f1af6"
      },
      "source": [
        "n = lambda_dim\n",
        "n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 287
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1HzoLhhVK1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# out very own softmax\n",
        "def output2probs(output):\n",
        "    output = np.dot(output, weights[0]) + weights[1]\n",
        "    output -= output.max()\n",
        "    output = np.exp(output)\n",
        "    output /= output.sum()\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV34VRogVK11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def output2probs1(output):\n",
        "    output0 = np.dot(output[:n//2], weights[0][:n//2,:])\n",
        "    output1 = np.dot(output[n//2:], weights[0][n//2:,:])\n",
        "    output = output0 + output1 # + output0 * output1\n",
        "    output += weights[1]\n",
        "    output -= output.max()\n",
        "    output = np.exp(output)\n",
        "    output /= output.sum()\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv7M6X3OVK15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lpadd(x, maxlend=maxlend, eos=eos):\n",
        "    \"\"\"left (pre) pad a description to maxlend and then add eos.\n",
        "    The eos is the input to predicting the first word in the headline\n",
        "    \"\"\"\n",
        "    assert maxlend >= 0\n",
        "    if maxlend == 0:\n",
        "        return [eos]\n",
        "    n = len(x)\n",
        "    if n > maxlend:\n",
        "        x = x[-maxlend:]\n",
        "        n = maxlend\n",
        "    return [empty]*(maxlend-n) + x + [eos]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkvuW0_5VK17",
        "colab_type": "code",
        "colab": {},
        "outputId": "fdbc4420-81a6-4255-e30d-13fbf46165eb"
      },
      "source": [
        "samples = [lpadd([3]*26)]\n",
        "# pad from right (post) so the first maxlend will be description followed by headline\n",
        "print(samples)\n",
        "data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
        "print(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1]]\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            "  3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoYIAk4OVK1_",
        "colab_type": "code",
        "colab": {},
        "outputId": "34c372f4-f94c-4794-928c-821623807922"
      },
      "source": [
        "np.all(data[:,maxlend] == eos)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQtBINfEVK2D",
        "colab_type": "code",
        "colab": {},
        "outputId": "ed132ac9-2882-4454-c3f2-285e7b013741"
      },
      "source": [
        "data.shape,list(map(len, samples))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1, 91), [76])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 293
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlT7T6wwVK2K",
        "colab_type": "code",
        "colab": {},
        "outputId": "80b2c705-4a2c-45fa-bc7d-02207ac45ff4"
      },
      "source": [
        "probs = model.predict(data, verbose=0, batch_size=1)\n",
        "probs.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 16, 48)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x81nCQUpVK2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# variation to https://github.com/ryankiros/skip-thoughts/blob/master/decoding/search.py\n",
        "def beamsearch(predict, start=[empty]*maxlend + [eos], avoid=None, avoid_score=1,\n",
        "               k=1, maxsample=maxlen, use_unk=True, oov=vocab_size-1, empty=empty, eos=eos, temperature=1.0):\n",
        "    \"\"\"return k samples (beams) and their NLL scores, each sample is a sequence of labels,\n",
        "    all samples starts with an `empty` label and end with `eos` or truncated to length of `maxsample`.\n",
        "    You need to supply `predict` which returns the label probability of each sample.\n",
        "    `use_unk` allow usage of `oov` (out-of-vocabulary) label in samples\n",
        "    \"\"\"\n",
        "    def sample(energy, n, temperature=temperature):\n",
        "        \"\"\"sample at most n different elements according to their energy\"\"\"\n",
        "        n = min(n,len(energy))\n",
        "        prb = np.exp(-np.array(energy) / temperature )\n",
        "        res = []\n",
        "        for i in range(n):\n",
        "            z = np.sum(prb)\n",
        "            r = np.argmax(np.random.multinomial(1, prb/z, 1))\n",
        "            res.append(r)\n",
        "            prb[r] = 0. # make sure we select each element only once\n",
        "        return res\n",
        "\n",
        "    dead_samples = []\n",
        "    dead_scores = []\n",
        "    live_samples = [list(start)]\n",
        "    live_scores = [0]\n",
        "\n",
        "    while live_samples:\n",
        "        # for every possible live sample calc prob for every possible label \n",
        "        probs = predict(live_samples, empty=empty)\n",
        "        assert vocab_size == probs.shape[1]\n",
        "\n",
        "        # total score for every sample is sum of -log of word prb\n",
        "        cand_scores = np.array(live_scores)[:,None] - np.log(probs)\n",
        "        cand_scores[:,empty] = 1e20\n",
        "        if not use_unk and oov is not None:\n",
        "            cand_scores[:,oov] = 1e20\n",
        "        if avoid:\n",
        "            for a in avoid:\n",
        "                for i, s in enumerate(live_samples):\n",
        "                    n = len(s) - len(start)\n",
        "                    if n < len(a):\n",
        "                        # at this point live_sample is before the new word,\n",
        "                        # which should be avoided, is added\n",
        "                        cand_scores[i,a[n]] += avoid_score\n",
        "        live_scores = list(cand_scores.flatten())\n",
        "        \n",
        "\n",
        "        # find the best (lowest) scores we have from all possible dead samples and\n",
        "        # all live samples and all possible new words added\n",
        "        scores = dead_scores + live_scores\n",
        "        ranks = sample(scores, k)\n",
        "        n = len(dead_scores)\n",
        "        dead_scores = [dead_scores[r] for r in ranks if r < n]\n",
        "        dead_samples = [dead_samples[r] for r in ranks if r < n]\n",
        "        \n",
        "        live_scores = [live_scores[r-n] for r in ranks if r >= n]\n",
        "        live_samples = [live_samples[(r-n)//vocab_size]+[(r-n)%vocab_size] for r in ranks if r >= n]\n",
        "\n",
        "        # live samples that should be dead are...\n",
        "        # even if len(live_samples) == maxsample we dont want it dead because we want one\n",
        "        # last prediction out of it to reach a headline of maxlenh\n",
        "        def is_zombie(s):\n",
        "            return s[-1] == eos or len(s) > maxsample\n",
        "        \n",
        "        # add zombies to the dead\n",
        "        dead_scores += [c for s, c in zip(live_samples, live_scores) if is_zombie(s)]\n",
        "        dead_samples += [s for s in live_samples if is_zombie(s)]\n",
        "        \n",
        "        # remove zombies from the living \n",
        "        live_scores = [c for s, c in zip(live_samples, live_scores) if not is_zombie(s)]\n",
        "        live_samples = [s for s in live_samples if not is_zombie(s)]\n",
        "\n",
        "    return dead_samples, dead_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9ihCDLiVK2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install python-Levenshtein"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvbzYw8SVK2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def keras_rnn_predict(samples, empty=empty, model=model, maxlen=maxlen):\n",
        "    \"\"\"for every sample, calculate probability for every possible label\n",
        "    you need to supply your RNN model and maxlen - the length of sequences it can handle\n",
        "    \"\"\"\n",
        "    sample_lengths = list(map(len, samples))\n",
        "    assert all(l > maxlend for l in sample_lengths)\n",
        "    assert all(l[maxlend] == eos for l in samples)\n",
        "    # pad from right (post) so the first maxlend will be description followed by headline\n",
        "    data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
        "    probs = model.predict(data, verbose=0, batch_size=batch_size)\n",
        "    return np.array([output2probs(prob[sample_length-maxlend-1]) for prob, sample_length in zip(probs, sample_lengths)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRRl6XQsVK2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vocab_fold(xs):\n",
        "    \"\"\"convert list of word indexes that may contain words outside vocab_size to words inside.\n",
        "    If a word is outside, try first to use glove_idx2idx to find a similar word inside.\n",
        "    If none exist then replace all accurancies of the same unknown word with <0>, <1>, ...\n",
        "    \"\"\"\n",
        "    xs = [x if x < vocab_size-nb_unknown_words else glove_idx2idx.get(x,x) for x in xs]\n",
        "    # the more popular word is <0> and so on\n",
        "    outside = sorted([x for x in xs if x >= vocab_size-nb_unknown_words])\n",
        "    # if there are more than nb_unknown_words oov words then put them all in nb_unknown_words-1\n",
        "    outside = dict((x,vocab_size-1-min(i, nb_unknown_words-1)) for i, x in enumerate(outside))\n",
        "    xs = [outside.get(x,x) for x in xs]\n",
        "    return xs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olYhBYTFVK2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vocab_unfold(desc,xs):\n",
        "    # assume desc is the unfolded version of the start of xs\n",
        "    unfold = {}\n",
        "    for i, unfold_idx in enumerate(desc):\n",
        "        fold_idx = xs[i]\n",
        "        if fold_idx >= vocab_size-nb_unknown_words:\n",
        "            unfold[fold_idx] = unfold_idx\n",
        "    return [unfold.get(x,x) for x in xs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCKqmMUcVK2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import Levenshtein\n",
        "\n",
        "def gensamples(X=None, X_test=None, Y_test=None, avoid=None, avoid_score=1, skips=2, k=10, batch_size=batch_size, short=True, temperature=1., use_unk=True):\n",
        "    if X is None or isinstance(X,int):\n",
        "        if X is None:\n",
        "            i = random.randint(0,len(X_test)-1)\n",
        "        else:\n",
        "            i = X\n",
        "        print('HEAD %d:'%i,' '.join(idx2word[w] for w in Y_test[i]))\n",
        "        print('DESC:',' '.join(idx2word[w] for w in X_test[i]))\n",
        "        sys.stdout.flush()\n",
        "        x = X_test[i]\n",
        "    else:\n",
        "        for w in X.split():\n",
        "            w = w.rstrip('^')\n",
        "            if not w in word2idx:\n",
        "                word2idx[w] = word2idx.get(w, len(word2idx))\n",
        "\n",
        "        x = [word2idx[w.rstrip('^')] for w in X.split()]\n",
        "        \n",
        "    if avoid:\n",
        "        # avoid is a list of avoids. Each avoid is a string or list of word indicies\n",
        "        if isinstance(avoid,str) or isinstance(avoid[0], int) or isinstance(avoid[0], np.int64):\n",
        "            avoid = [avoid]\n",
        "        avoid = [a.split() if isinstance(a,str) else a for a in avoid]\n",
        "        avoid = [vocab_fold([w if isinstance(w,int) or isinstance(w,np.int64) else word2idx[w] for w in a])\n",
        "                 for a in avoid]\n",
        "\n",
        "    print('HEADS:')\n",
        "    samples = []\n",
        "    if maxlend == 0:\n",
        "        skips = [0]\n",
        "    else:\n",
        "        skips = range(min(maxlend,len(x)), max(maxlend,len(x)), abs(maxlend - len(x)) // skips + 1)\n",
        "    for s in skips:\n",
        "        start = lpadd(x[:s])\n",
        "        fold_start = vocab_fold(start)\n",
        "        sample, score = beamsearch(predict=keras_rnn_predict, start=fold_start, avoid=avoid, avoid_score=avoid_score,\n",
        "                                   k=k, temperature=temperature, use_unk=use_unk)\n",
        "        assert all(s[maxlend] == eos for s in sample)\n",
        "        samples += [(s,start,scr) for s,scr in zip(sample,score)]\n",
        "\n",
        "    samples.sort(key=lambda x: x[-1])\n",
        "    codes = []\n",
        "    for sample, start, score in samples:\n",
        "        code = ''\n",
        "        words = []\n",
        "        sample = vocab_unfold(start, sample)[len(start):]\n",
        "        for w in sample:\n",
        "            if w == eos:\n",
        "                break\n",
        "            words.append(idx2word[w])\n",
        "            code += chr(w//(256*256)) + chr((w//256)%256) + chr(w%256)\n",
        "        if short:\n",
        "            distance = min([100] + [-Levenshtein.jaro(code,c) for c in codes])\n",
        "            if distance > -0.6:\n",
        "                print(score, ' '.join(words))\n",
        "        #         print '%s (%.2f) %f'%(' '.join(words), score, distance)\n",
        "        else:\n",
        "                print(score, ' '.join(words))\n",
        "        codes.append(code)\n",
        "    return samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm4awCRMVK2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 8\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm6Kd9PvVK2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X = \"* Billy Joel is looking for a buyer in Sagaponack^ . Now that he and wife Katie Lee Joel are splitting up , the singer is planning to sell the two oceanfront^ properties he bought for her in 2007 . The four-bedroom mansion ( No . 1 ) and smaller beach bungalow^ ( No . 2 ) will be listed with Corcoran 's Biana^ Stepanian^ for a combined $ 35 million . * Richard Bressler^ , the former CFO of Viacom and now a managing\"\n",
        "# Y = \"Billy Joel Lists in Sagaponack^\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9a4EE1uVK2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_file  = 'pickles/json_news_5000.pickle'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQtDeaV4VK2p",
        "colab_type": "code",
        "colab": {},
        "outputId": "2c351159-284a-4830-a0bc-6e36ee8484d4"
      },
      "source": [
        "# Read from eval pickle file\n",
        "import pickle\n",
        "import csv\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "#rouge = Rouge()\n",
        "\n",
        "with open(data_file, 'rb') as fp:\n",
        "    df = pickle.load(fp) # keywords are not used in this project\n",
        "    heads = df['title'].tolist()[:10]\n",
        "    desc = df['content'].tolist()[:10]\n",
        "\n",
        "with open('5k-2-eval.csv', 'w') as fd:\n",
        "    writer = csv.writer(fd)\n",
        "    writer.writerow(['Reference', 'Description', 'Generated', 'BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4'])\n",
        "\n",
        "    for i in range(len(heads)):\n",
        "        Y = heads[i][0]\n",
        "        X = desc[i][0]\n",
        "\n",
        "        print('HEAD: ', Y)\n",
        "        try:\n",
        "            samples = gensamples(X=X, skips=2, batch_size=batch_size, k=10, temperature=1.)\n",
        "            reference = [Y.split()]\n",
        "\n",
        "            headline = samples[0][0][len(samples[0][1]):]\n",
        "\n",
        "            candidate = [idx2word[w] for w in headline]\n",
        "            candidate = candidate[:len(candidate)-1]\n",
        "            head_str = ' '.join(idx2word[w] for w in headline[:len(headline)-1])\n",
        "\n",
        "            # BLEU score\n",
        "            bleu1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
        "            bleu2 = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))\n",
        "            bleu3 = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0))\n",
        "            bleu4 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "            print('Cumulative 1-gram: %f' % bleu1)\n",
        "            print('Cumulative 2-gram: %f' % bleu2)\n",
        "            print('Cumulative 3-gram: %f' % bleu3)\n",
        "            print('Cumulative 4-gram: %f' % bleu4)\n",
        "\n",
        "            writer.writerow([Y, X, head_str, bleu1, bleu2, bleu3, bleu4])\n",
        "        except:\n",
        "            print('Failed to generate headlines.')\n",
        "            writer.writerow([Y, X, 'N/A', 0, 0, 0, 0])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HEAD:  Worcester breakfast club for veteran give hunger it marching order\n",
            "HEADS:\n",
            "2.8008017539978027 \n",
            "9.256567478179932 ago\n",
            "34.395591497421265 fed more suri say\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "HEAD:  Jumpshot Gives Marketers Renewed Visibility Into Paid and Organic Keywords With Launch of Jumpshot Elite\n",
            "HEADS:\n",
            "54.79575490951538 pacific link sept to per to announced\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "HEAD:  The Return Of The Nike Air Max Sensation Has 80 s Babies Hyped\n",
            "HEADS:\n",
            "128.1415295600891 33 alarm taking moves fifth keeper council moped rat rise threats express couldn\n",
            "142.07769012451172 increase in 23 legal million baby of lock commander 2016 rate steal fremantle labor potential fisherman\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "HEAD:  This New Dating App Will Ruin Your Internet Game\n",
            "HEADS:\n",
            "165.0010175704956 august dishes nambour sources economical probable rivers blind multani maiden temporary predecessor worldwide stunned hood ansari\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "HEAD:  Pay up or face legal action DBKL\n",
            "HEADS:\n",
            "44.38558602333069 is for s in a 2015 to from\n",
            "54.904032945632935 28 rays in partnership sacked of week\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "HEAD:  Euro up USD Pound and Yen down\n",
            "HEADS:\n",
            "28.35790467262268 and cost gidado\n",
            "58.830843448638916 at big operation by from solution with and of\n",
            "Cumulative 1-gram: 0.087866\n",
            "Cumulative 2-gram: 0.152188\n",
            "Cumulative 3-gram: 0.183439\n",
            "Cumulative 4-gram: 0.200291\n",
            "HEAD:  THE INFLUENCE OF OUR WORDS\n",
            "HEADS:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3.2151777744293213 \n",
            "10.338221788406372 season\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "HEAD:  Hands on with Apple s iPhone 6 iPad Pro and Apple TV\n",
            "HEADS:\n",
            "11.456171751022339 austin\n",
            "13.20128345489502 northwest\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "HEAD:  Harwood Feffer LLP Announces Investigation of VASCO Data Security International Inc\n",
            "HEADS:\n",
            "64.84104442596436 opening subscriptions leadership 2006 share you completely\n",
            "68.19050002098083 and cash due operation safety name ever is\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "HEAD:  What will happen to Peta Credlin and Brian Loughnane Liberal power couple\n",
            "HEADS:\n",
            "9.387882232666016 from\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okHNIJLIVK2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = \"This article is part of a feature we also send out via email as Politics  Policy Daily, a daily roundup of events and ideas in American politics written specially for newsletter subscribers. To sign up, please enter your email address in the field provided here.     During his first press conference in several months,   Donald Trump vehemently denied Tuesday‚Äôs reports alleging that Russia had compromising information about him, calling it ‚Äúfake news. ‚Äù Trump also announced that his sons will take over the Trump Organization once he becomes president, a move ethics experts say doesn‚Äôt satisfy    concerns and Trump again refused to release his tax returns, saying ‚Äúthe only ones who care about my tax returns are reporters. ‚Äù Representative John Lewis and Senator Corey Booker testified against Senator Jeff Sessions during the second day of his Senate confirmation hearing to serve as U. S. attorney general. Rex Tillerson, Trump‚Äôs choice for secretary of state, was critical of Russia during his hearing, saying the country ‚Äúposes a danger‚Äù to the United States and transportation secretary nominee Elaine Chao said she hopes to ‚Äúunleash the potential‚Äù of private investment for federal infrastructure projects. Seek the Truth: ‚ÄúBuzzFeed‚Äôs decision to publish a dossier full of serious accusations against   Donald Trump on Tuesday raised serious questions,‚Äù writes David A. Graham, one of which concerns the journalism ethics of publishing a document full of unverified claims. A Plan in Jeopardy?: Republicans are facing a number of obstacles in their quest to repeal the Affordable Care Act, such as finding a suitable alternative that won‚Äôt leave millions of Americans without insurance. Still, the GOP is moving forward with its plan\"# ‚Äî  for now. (Russell Berman) Divided We Fall: Trump‚Äôs critics will need to come together to minimize the damage they fear he will impose, argues Conor Friedersdorf. ‚ÄúYet large swaths of the right and left, including extremely thoughtful,   observers of the American scene, are behaving as if such cooperation is impossible. ‚Äù Why? Follow stories throughout the day with our Politics  Policy portal. More on Trump‚Äôs Business Plans: The   says he will turn all business operations over to his two sons once he assumes the presidency and won‚Äôt enter into any new deals with international partners. But ethics experts say Trump is still facing ‚Äúa constitutional crisis. ‚Äù (Maggie Haberman, Julie Hirschfeld Davis, and Eric Lipton, The New York Times) On the Defense: The cyber hack on the Democratic Party was instigated by an ‚Äúelite‚Äù unit of hackers ‚Äúlinked to the Russian military intelligence service, known as the GRU, and its targets span the globe and parallel the interests of the Russian state. ‚Äù The party never stood a chance against them. (Tim Johnson, McClatchy DC) A Look Inside: Marie Claire spoke with former ‚Äúskin chicks,‚Äù female members of the white supremacist movement. While the cause is   feelings of rage and a desire for community may be contributing to a growing number of women joining the movement. (Kate Storey) Thanks, Obama: Although reviews of his presidency will be mixed, writes Ezra Klein, Americans will miss several things about Barack Obama, namely ‚Äúhis decency. His   administration. The seriousness with which he approached his job. The faith he had in the American political system, and in Americans. ‚Äù (Vox) Solving a Mystery: Raheel Siddiqui, a Muslim Marine recruit, fell to his death during basic training in Parris Island, South Carolina, in March 2016. The Marine Corps ruled the death a suicide, but Siddiqui‚Äôs family thinks differently. Did a night of hazing go wrong? (Alex French, Esquire)   Before and After: Serving as commander in chief of the United States has taken a visible toll on President Obama. Click on these images to see how the president has aged over the past eight years. (Alan Taylor, The Atlantic) Senate confirmation hearings for   Trump‚Äôs Cabinet nominees began on Tuesday. What‚Äôs the one question you‚Äôd be afraid to answer honestly at your own confirmation hearing? Send your answers to hello@theatlantic. com, and our favorites will be featured in Friday‚Äôs Politics  Policy Daily.   by Elaine Godfrey (@elainejgodfrey) and Candice Norwood (@cjnorwoodwrites)\"\"\n",
        "Y = \"The Atlantic  Politics & Policy Daily: Back-to-Back Sessions\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sTEm_lnVK2u",
        "colab_type": "code",
        "colab": {},
        "outputId": "df65f2bb-66a7-4e1c-ab84-3843887063b3"
      },
      "source": [
        "samples = gensamples(X=X, skips=2, batch_size=batch_size, k=10, temperature=1.)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HEADS:\n",
            "44.14431929588318 center global is camp sept to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f75LLdfVVK2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = \"18 Cake GIFs That 'll Make You Moist\"\n",
        "Y = \"Is it 350degF^ in here or is it just me ?\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Our18QUVK2y",
        "colab_type": "code",
        "colab": {},
        "outputId": "be37e4ca-310a-4c1d-b2cf-b207b62063da"
      },
      "source": [
        "samples = gensamples(X, skips=2, batch_size=batch_size, k=10, temperature=1.)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HEADS:\n",
            "20.307226181030273 stake on\n",
            "36.96908521652222 park corporation 6 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1xXw2taVK20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = \"President Barack Obama 's re-election campaign is fundraising off of comments on Obama 's birth certificate by Mitt Romney 's son Matt .\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUI2e0ScVK21",
        "colab_type": "code",
        "colab": {},
        "outputId": "458d5d70-4583-4516-b412-efae1fb060df"
      },
      "source": [
        "gensamples(X, skips=2, batch_size=batch_size, k=10, temperature=1, use_unk=True, short=False);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HEADS:\n",
            "3.6473581790924072 \n",
            "14.61897349357605 new for\n",
            "19.983611822128296 new for on\n",
            "22.538193941116333 new for owner\n",
            "24.320326805114746 stream 2015 b\n",
            "25.679430961608887 stream 2015 increased\n",
            "28.41639542579651 new for on 34\n",
            "28.72929072380066 new for on follow\n",
            "30.03572654724121 stream 2015 b new\n",
            "31.766599893569946 new for on during to\n",
            "38.62402892112732 stream 2015 b new flood\n",
            "40.249629974365234 new for on during to man\n",
            "47.04489350318909 stream 2015 b new by with be\n",
            "74.07227563858032 new for on during to man concord wish in alamos\n",
            "87.7929995059967 stream 2015 b new by with be september alexa fuel is hits\n",
            "93.16409015655518 stream 2015 b new by with be september alexa fuel is hits with\n",
            "112.76234197616577 stream 2015 b new by with be september alexa fuel is hits with with cut blatter\n",
            "113.71352529525757 stream 2015 b new by with be september alexa fuel is hits with with cut yardley\n",
            "113.98764848709106 stream 2015 b new by with be september alexa fuel is hits with with cut doorview\n",
            "130.99231576919556 new for on during to man concord wish in alamos meeting 16 fut yoenis manager trendy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhgmHCOmVK23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = \"What have you been listening to this year ? If you want to find out using cold , hard evidence , then Spotify 's new Year in Music tool will tell you .\"\n",
        "Y = \"Spotify Will Make You Smarter for Your App\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga9NB2PkVK25",
        "colab_type": "code",
        "colab": {},
        "outputId": "1b64c4f3-f266-498b-fe8e-d34a0ab267af"
      },
      "source": [
        "samples = gensamples(X, skips=2, batch_size=batch_size, k=10, temperature=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HEADS:\n",
            "16.336666584014893 of music\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KqfSFqfVK2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headline = samples[0][0][len(samples[0][1]):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8fKJIKlVK3G",
        "colab_type": "code",
        "colab": {},
        "outputId": "4f95a4c1-e082-4ac7-af71-67bb0a630913"
      },
      "source": [
        "' '.join(idx2word[w] for w in headline)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'of music ~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 314
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAFuHSDrVK3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avoid = headline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu27xGa9VK3L",
        "colab_type": "code",
        "colab": {},
        "outputId": "79556ec4-5c2d-4765-e5d7-36a1256f1430"
      },
      "source": [
        "samples = gensamples(X, avoid=avoid, avoid_score=.1, skips=2, batch_size=batch_size, k=10, temperature=1.)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HEADS:\n",
            "14.983952617645263 the off\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWnnHJsWVK3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avoid = samples[0][0][len(samples[0][1]):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKFzmwZsVK3O",
        "colab_type": "code",
        "colab": {},
        "outputId": "8032dfe4-5748-46ab-f3f5-525fb270e81a"
      },
      "source": [
        "samples = gensamples(X, avoid=avoid, avoid_score=.1, skips=2, batch_size=batch_size, k=10, temperature=1.)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HEADS:\n",
            "13.506104564666748 s in\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXj-nOiwVK3Q",
        "colab_type": "code",
        "colab": {},
        "outputId": "76587999-a5cd-4add-f40b-e990501c0790"
      },
      "source": [
        "len(samples)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jktv6iDFVK3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wsimple_context(X, mask, n=activation_rnn_size, maxlend=maxlend, maxlenh=maxlenh):\n",
        "    desc, head = X[:,:maxlend,:], X[:,maxlend:,:]\n",
        "    head_activations, head_words = head[:,:,:n], head[:,:,n:]\n",
        "    desc_activations, desc_words = desc[:,:,:n], desc[:,:,n:]\n",
        "    \n",
        "    # RTFM http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor.batched_tensordot\n",
        "    # activation for every head word and every desc word\n",
        "    activation_energies = K.batch_dot(head_activations, desc_activations, axes=([2],[2]))\n",
        "    # make sure we dont use description words that are masked out\n",
        "    #assert mask.ndim == 2\n",
        "    activation_energies = activation_energies + -1e20*K.expand_dims(1.-K.cast(mask[:, :maxlend],'float32'),1)\n",
        "    \n",
        "    # for every head word compute weights for every desc word\n",
        "    activation_energies = K.reshape(activation_energies,(-1,maxlend))\n",
        "    activation_weights = K.softmax(activation_energies)\n",
        "    activation_weights = K.reshape(activation_weights,(-1,maxlenh,maxlend))\n",
        "\n",
        "    #eturn activation_weights\n",
        "    desc_avg_word = K.batch_dot(activation_weights, desc_words, axes=(2,1))\n",
        "    return K.concatenate((desc_avg_word, head_words))\n",
        "\n",
        "class WSimpleContext(Lambda):\n",
        "    def __init__(self):\n",
        "        super(WSimpleContext, self).__init__(wsimple_context)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return input_mask[:, maxlend:]\n",
        "    \n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        nb_samples = input_shape[0]\n",
        "        n = lambda_dim\n",
        "        return (nb_samples, maxlenh, n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO8hvsj_VK3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wmodel = Sequential()\n",
        "wmodel.add(rnn_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qywXc1eEVK3a",
        "colab_type": "code",
        "colab": {},
        "outputId": "9df71048-28a8-40f0-e03c-657d125ccc66"
      },
      "source": [
        "rnn_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 91, 100)           3945300   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 91, 64)            42240     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 91, 64)            0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 91, 64)            33024     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 91, 64)            0         \n",
            "=================================================================\n",
            "Total params: 4,020,564\n",
            "Trainable params: 4,020,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPAXPFWDVK3b",
        "colab_type": "code",
        "colab": {},
        "outputId": "bc94c34a-82ee-4c87-a79c-adfd234e79bb"
      },
      "source": [
        "wmodel.add(Lambda(simple_context,\n",
        "                 mask = lambda inputs, mask: mask[:,maxlend:],\n",
        "                 output_shape = lambda input_shape: (input_shape[0], maxlenh, lambda_dim)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.framework.ops.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkOUEgvMVK3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeUi1kQpVK3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 8\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoD7NohTVK3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = \"What have you been listening to this year ? If you want to find out using cold , hard evidence , then Spotify 's new Year in Music tool will tell you .\"\n",
        "Y = \"Spotify Will Make You Smarter for Your App\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRSNlzvWVK3m",
        "colab_type": "code",
        "colab": {},
        "outputId": "d57fba55-2a16-4979-cbf5-1abc96e36de5"
      },
      "source": [
        "samples = gensamples(X, skips=2, batch_size=batch_size, k=10, temperature=1.)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HEADS:\n",
            "6.27395486831665 \n",
            "16.794414043426514 to and in\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ifkwy89KVK3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = samples[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGWe5V0iVK3q",
        "colab_type": "code",
        "colab": {},
        "outputId": "10cd8ffd-b05d-4bb6-b53c-1ae37d7ef14b"
      },
      "source": [
        "' '.join([idx2word[w] for w in sample])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <2>^ have you been listening to this year <3>^ <4>^ you want to find out using cold <6>^ hard evidence <6>^ then <7>^ <0>^ new <8>^ in <9>^ tool will tell you <1>^ ~ ~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 329
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2zadRFJVK3s",
        "colab_type": "code",
        "colab": {},
        "outputId": "405f2f74-ac02-44b5-8646-a1eb43dfb398"
      },
      "source": [
        "data = sequence.pad_sequences([sample], maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 91)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 330
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N2tf7NSVK3v",
        "colab_type": "code",
        "colab": {},
        "outputId": "a5cc410e-4a00-4130-a0fd-694f0bf76dd0"
      },
      "source": [
        "weights = wmodel.predict(data, verbose=0, batch_size=1)\n",
        "weights.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 16, 48)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 331
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gnOmawaVK3x",
        "colab_type": "code",
        "colab": {},
        "outputId": "1d44a997-cc83-4860-a26b-64c76aa54dac"
      },
      "source": [
        "startd = np.where(data[0,:] != empty)[0][0]\n",
        "try:\n",
        "    lenh = np.where(data[0,maxlend+1:] == eos)[0][0]\n",
        "except:\n",
        "    print('No EOS in data[0,maxlend+1:], using data[0,maxlend+1:].shape[0] instead')\n",
        "    lenh = data[0, maxlend+1:].shape[0]\n",
        "startd, lenh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 332
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OrAH1S5VK3y",
        "colab_type": "code",
        "colab": {},
        "outputId": "04669ce6-847c-4a25-8d3f-d0c9c43e891f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.hist(np.array(weights[0,:lenh,startd:].flatten()+1), bins=100);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x1a31202c88>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yady-671VK30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "def heat(sample,weights,dark=0.3):\n",
        "    weights = (weights - weights.min())/(weights.max() - weights.min() + 1e-4)\n",
        "    html = ''\n",
        "    fmt = ' <span style=\"background-color: #{0:x}{0:x}ff\">{1}</span>'\n",
        "    for t,w in zip(sample,weights):\n",
        "        c = int(256*((1.-dark)*(1.-w)+dark))\n",
        "        html += fmt.format(c,idx2word[t])\n",
        "    display(HTML(html))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx3PffauVK33",
        "colab_type": "code",
        "colab": {},
        "outputId": "dfa7534b-934a-4350-dda8-69dfc521b0f3"
      },
      "source": [
        "heat(sample, weights[0,-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              " <span style=\"background-color: #9090ff\">_</span> <span style=\"background-color: #d9d9ff\">_</span> <span style=\"background-color: #7b7bff\">_</span> <span style=\"background-color: #d6d6ff\">_</span> <span style=\"background-color: #e5e5ff\">_</span> <span style=\"background-color: #ceceff\">_</span> <span style=\"background-color: #7d7dff\">_</span> <span style=\"background-color: #d2d2ff\">_</span> <span style=\"background-color: #fafaff\">_</span> <span style=\"background-color: #8c8cff\">_</span> <span style=\"background-color: #7575ff\">_</span> <span style=\"background-color: #7a7aff\">_</span> <span style=\"background-color: #4f4fff\">_</span> <span style=\"background-color: #bebeff\">_</span> <span style=\"background-color: #8181ff\">_</span> <span style=\"background-color: #7777ff\">_</span> <span style=\"background-color: #8080ff\">_</span> <span style=\"background-color: #7979ff\">_</span> <span style=\"background-color: #cdcdff\">_</span> <span style=\"background-color: #b5b5ff\">_</span> <span style=\"background-color: #fafaff\">_</span> <span style=\"background-color: #f4f4ff\">_</span> <span style=\"background-color: #8989ff\">_</span> <span style=\"background-color: #a9a9ff\">_</span> <span style=\"background-color: #4e4eff\">_</span> <span style=\"background-color: #e8e8ff\">_</span> <span style=\"background-color: #ffffff\">_</span> <span style=\"background-color: #6a6aff\">_</span> <span style=\"background-color: #7676ff\">_</span> <span style=\"background-color: #6262ff\">_</span> <span style=\"background-color: #ffffff\">_</span> <span style=\"background-color: #5656ff\">_</span> <span style=\"background-color: #ffffff\">_</span> <span style=\"background-color: #ddddff\">_</span> <span style=\"background-color: #fbfbff\">_</span> <span style=\"background-color: #6b6bff\">_</span> <span style=\"background-color: #4c4cff\">_</span> <span style=\"background-color: #c5c5ff\">_</span> <span style=\"background-color: #100100ff\">_</span> <span style=\"background-color: #8585ff\">_</span> <span style=\"background-color: #6464ff\">_</span> <span style=\"background-color: #6565ff\">_</span> <span style=\"background-color: #4d4dff\"><2>^</span> <span style=\"background-color: #c9c9ff\">have</span> <span style=\"background-color: #f8f8ff\">you</span> <span style=\"background-color: #c7c7ff\">been</span> <span style=\"background-color: #8888ff\">listening</span> <span style=\"background-color: #5151ff\">to</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LY4WOlsVK37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}